{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../assets/images/Cover.png\" alt=\"Cover\" title=\"AI2E Cover\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI2E - [Workshop 11 ] - [RL : Q-learning]- Self-Driving Cab\n",
    "\n",
    "Let's design a simulation of a self-driving cab. The major goal is to demonstrate, in a simplified environment, how we can use RL techniques to develop an efficient and safe approach for tackling this problem.\n",
    "\n",
    "<br/> The Smartcab's job is to pick up the passenger at one location and drop him off in another. Here are a few things that we'd love our Smartcab to take care of:\n",
    "<ul>\n",
    "    <li>Drop off the passenger to the right location.</li>\n",
    "    <li>Save passenger's time by taking minimum time possible to drop off. </li>\n",
    "    <li>Take care of passenger's safety and traffic rules ?</li>\n",
    "    <li>There are different aspects that need to be considered here while modeling an RL solution to this problem: rewards, states, and actions.</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "## Content \n",
    "1. What's OpenAI Gym\n",
    "2. Taxi-v3 environment\n",
    "3. Q-Learning conception\n",
    "<br/> 3.1. Rewards\n",
    "<br/> 3.2. State space\n",
    "<br/> 3.2. Action space\n",
    "4. Q-Learning implementation\n",
    "<br/> 4.1. Agent Training\n",
    "<br/> 4.2. Agent Evaluating\n",
    "5. Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's OpenAI Gym\n",
    "**OpenAI** is an artificial intelligence research organization that aims to promote and develop artificial general intelligence in ways that benefit humanity as a whole. \n",
    "<br/> For our purposes, we will be using OpenAI Gym, which is a toolkit for developing and comparing reinforcement learning algorithms. Gym will provide the environment, Taxi-v3, for us to train our agents: Q-learning;\n",
    "We will use the following functions:\n",
    "<ul>\n",
    "<li>env.reset: resets the environment and returns a random initial state.</li>\n",
    "<li>env.step(action): advances the environment by one timestep.</li>\n",
    "<li>env.render: renders one frame of environment (helpful in visualization)</li>\n",
    "\n",
    "<li>The environment returns:\n",
    "    <ul>\n",
    "<li>observation: an environment-specific object representing your observation of the environment </li>\n",
    "<li>reward: the amount of reward/score achieved by the previous action </li>\n",
    "<li>done: indicates whether it is time to reset the environment again, i.e., the agent has achieved the goal </li>\n",
    "<li>info: diagnostic info such as performance and latency useful for debugging purposes</li>\n",
    "    </ul>\n",
    "</li>\n",
    "</ul>\n",
    "In each timestep, our agent performs a specific action, and the environment returns the observation and a reward as a consequence of performing that particular action in the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taxi-v3 environement\n",
    "This task was introduced in [Dietterich2000] to illustrate some issues in hierarchical reinforcement learning. There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop him off in another. You receive +20 points for a successful dropoff, and lose 1 point for every timestep it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning conception\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rewards\n",
    "Since the agent (the imaginary driver) is reward-motivated and is going to learn how to control the cab by trial experiences in the environment, we need to decide the rewards and/or penalties and their magnitude accordingly. Here a few points to consider:\n",
    "<ul>\n",
    "    <li> The agent should receive a high positive reward for a successful dropoff because this behavior is highly desired </li>\n",
    "    <li> The agent should be penalized if it tries to drop off a passenger in wrong locations </li>\n",
    "    <li> The agent should get a slight negative reward for not making it to the destination after every time-step. \"Slight\" negative because we would prefer our agent to reach late instead of making wrong moves trying to reach to the destination as fast as possible  </li>\n",
    "\n",
    "</ul>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### State Space\n",
    "The state should contain useful information the agent needs to make the right action\n",
    "\n",
    "<br/> The State Space is the set of all possible situations our taxi could inhabit, Let's assume Smartcab is the only vehicle in this parking lot. We can break up the parking lot into a 5x5 grid, which gives us 25 possible taxi locations. These 25 locations are one part of our state space.\n",
    "\n",
    "<br/> Notice the current location state of our taxi is coordinate (3, 1).\n",
    "\n",
    "<br/> You'll also notice there are four (4) locations that we can pick up and drop off a passenger represented by **R**, **G**, **Y**, **B** or [(0,0), (0,4), (4,0), (4,3)] in (row, col) coordinates. Our illustrated passenger is in location Y and they wish to go to location R.\n",
    "\n",
    "<br/> When we also account for one (1) additional passenger state of being inside the taxi, we can take all combinations of passenger locations and destination locations to come to a total number of states for our taxi environment; there's four (4) destinations and five (4 + 1) passenger locations.\n",
    "\n",
    "<br/> **So, our taxi environment has 5×5×5×4=500 total possible states**.\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div>\n",
    "<img src=\"../assets/images/Self-Driving Cab.png\" alt=\"Self-Driving Cab\" title=\"Self-Driving Cab\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Action Space\n",
    "The agent encounters one of the 500 states and it takes an action. The action in our case can be to move in a direction or decide to pickup/dropoff a passenger.\n",
    "\n",
    "<br/> In other words, we have six possible actions:\n",
    "<ul>\n",
    "    <li> Move south </li> \n",
    "    <li> Move north </li> \n",
    "    <li> Move east </li> \n",
    "    <li> Move west </li> \n",
    "    <li> Pickup </li> \n",
    "    <li> Dropoff </li> \n",
    "</ul>\n",
    "    \n",
    "<br/> This is the action space: the set of all the actions that our agent can take in a given state.\n",
    "\n",
    "You'll notice in the illustration above, that the taxi cannot perform certain actions in certain states due to walls. In environment's code, we will simply provide a -1 penalty for every wall hit and the taxi won't move anywhere. This will just rack up penalties causing the taxi to consider going around the wall."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q-Learning implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install cmake gym[atari] scipy\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Environement"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v2\").env \n",
    "#env.render()\n",
    "#env.reset() # reset environment to a new, random state\n",
    "#env.render() \n",
    "\n",
    "#print(\"Action Space {}\".format(env.action_space))\n",
    "#print(\"State Space {}\".format(env.observation_space))\n",
    "\n",
    "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "env.s = state\n",
    "env.render()\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "####  Agent Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    frames = [] # for animation\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        # Put each rendered frame into dict for animation\n",
    "        frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward\n",
    "            }\n",
    "        )\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "print_frames(frames)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Agent Evaluating"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])   #Using Q table only\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "Summarize your notebook. Reiterate the lesson and important takeaways. \n",
    "\n",
    "In the next, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cmake in /Users/admin/opt/anaconda3/lib/python3.7/site-packages (3.17.2)\n",
      "Requirement already satisfied: gym[atari] in /Users/admin/opt/anaconda3/lib/python3.7/site-packages (0.8.2)\n",
      "Requirement already satisfied: scipy in /Users/admin/opt/anaconda3/lib/python3.7/site-packages (1.4.1)\n",
      "Requirement already satisfied: requests>=2.0 in /Users/admin/opt/anaconda3/lib/python3.7/site-packages (from gym[atari]) (2.23.0)\n",
      "Requirement already satisfied: pyglet>=1.2.0 in /Users/admin/opt/anaconda3/lib/python3.7/site-packages (from gym[atari]) (1.2.4)\n",
      "Requirement already satisfied: six in /Users/admin/opt/anaconda3/lib/python3.7/site-packages (from gym[atari]) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /Users/admin/opt/anaconda3/lib/python3.7/site-packages (from gym[atari]) (1.18.2)\n",
      "Requirement already satisfied: atari-py>=0.0.21; extra == \"atari\" in /Users/admin/opt/anaconda3/lib/python3.7/site-packages (from gym[atari]) (0.2.6)\n",
      "Requirement already satisfied: PyOpenGL; extra == \"atari\" in /Users/admin/opt/anaconda3/lib/python3.7/site-packages (from gym[atari]) (3.1.5)\n",
      "Requirement already satisfied: Pillow; extra == \"atari\" in /Users/admin/opt/anaconda3/lib/python3.7/site-packages (from gym[atari]) (6.2.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/admin/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.0->gym[atari]) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/admin/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.0->gym[atari]) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/admin/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.0->gym[atari]) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/admin/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.0->gym[atari]) (1.25.8)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1 is available.\n",
      "You should consider upgrading via the '/Users/admin/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install cmake gym[atari] scipy\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-05-04 14:32:40,541] Making new env: Taxi-v2\n",
      "/Users/admin/opt/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 328\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v2\").env \n",
    "#env.render()\n",
    "#env.reset() # reset environment to a new, random state\n",
    "#env.render() \n",
    "\n",
    "#print(\"Action Space {}\".format(env.action_space))\n",
    "#print(\"State Space {}\".format(env.observation_space))\n",
    "\n",
    "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "env.s = state\n",
    "env.render()\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Agent Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.StringIO object at 0x11e562050>\n",
      "Timestep: 14\n",
      "State: 97\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    frames = [] # for animation\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        # Put each rendered frame into dict for animation\n",
    "        frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward\n",
    "            }\n",
    "        )\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.StringIO object at 0x11e562050>\n",
      "Timestep: 14\n",
      "State: 97\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 12.61\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])   #Using Q table only\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "Summarize your notebook. Reiterate the lesson and important takeaways. \n",
    "\n",
    "In the next, ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}