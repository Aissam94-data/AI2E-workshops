{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../assets/images/Cover.png\" alt=\"Cover\" title=\"AI2E Cover\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI2E - [Workshop 2] - [Data exploration] \n",
    "\n",
    "We will go through the essential steps to explore and get the most benefit from our data. \n",
    "\n",
    "### Content \n",
    "1. Get started with data\n",
    "2. Feature selection\n",
    "3. Feature engineering\n",
    "4. Trait missing values\n",
    "5. Data visualisation\n",
    "6. Handle outliers\n",
    "7. Encode data\n",
    "8. Scaling\n",
    "9. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get started with data:\n",
    "The dataset provided by an algerian company includes variables about adress of depart, adress of arrival, distance, ... \n",
    "    The training dataset provided here is a subset of over 60,000 samples.\n",
    "\n",
    "#### variables description\n",
    "<img src=\"../assets/images/w2_Vdesc.PNG\" title=\"variables description\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read the data\n",
    "df = pd.read_csv('data/vtc_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get information about integer values\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>std: </b>the standard deviation is a measure of the amount of variation or dispersion of a set of values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get an overall look about the data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the duration of the whole data\n",
    "print(\"First date : \", df[\"date_of_travel\"].min())\n",
    "print(\"Last date : \", df[\"date_of_travel\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature selection\n",
    "Feature Selection is one of the core concepts in machine learning which hugely impacts the performance of your model. The data features that you use to train your machine learning models have a huge influence on the performance you can achieve.  \n",
    "<b> Irrelevant or partially relevant features can negatively impact model performance. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exo: Define our features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_features = []\n",
    "target_name = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[chosen_features]\n",
    "labels = df[target_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature engineering\n",
    "Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add hour and day columns\n",
    "\n",
    "features['date_of_travel'] = pd.DataFrame(pd.to_datetime(features.date_of_travel, format=\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "# create a new column\n",
    "features[\"hour\"] = features[\"date_of_travel\"].dt.hour\n",
    "features[\"day_name\"] = features[\"date_of_travel\"].dt.day_name()\n",
    "# drop the \"date of travel\" column\n",
    "features.drop([\"date_of_travel\"], axis = 1, inplace = True)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the lat and lon columns\n",
    "features[['lat','lon']] = features.lat_and_long_of_arrival_address.str.split(\",\", expand=True)\n",
    "features[\"lat\"] = pd.to_numeric(features[\"lat\"], downcast=\"float\")\n",
    "features[\"lon\"] = pd.to_numeric(features[\"lon\"], downcast=\"float\")\n",
    "features.drop([\"lat_and_long_of_arrival_address\"], axis = 1, inplace = True)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Trait missing values\n",
    "Missing values are one of the most common problems you can encounter when you try to prepare your data for machine learning. The reason for the missing values might be human errors, interruptions in the data flow, privacy concerns, and so on. Whatever is the reason, missing values affect the performance of the machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are null values in each column\n",
    "for col in features.columns :\n",
    "    print(col,':' ,features[col].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trait null values\n",
    "features[\"distance\"].fillna(features[\"distance\"].mean(), inplace = True)\n",
    "features[\"lat\"].fillna(features[\"lat\"].mean(), inplace = True)\n",
    "features[\"lon\"].fillna(features[\"lon\"].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exo: confirm that we don't have null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data visualisation\n",
    "Data visualization is the graphic representation of data. It involves producing images that communicate relationships among the represented data to viewers of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap to show correlation\n",
    "import seaborn as sns\n",
    "# we need to use the labels column\n",
    "corr = pd.concat([features, labels], axis = 1).corr()\n",
    "sns.heatmap(corr, \n",
    "        xticklabels=corr.columns,\n",
    "        yticklabels=corr.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>correlation: </b> refers to the degree to which a pair of variables are linearly related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print a chart of the estimated time in function of the distance\n",
    "plt.scatter(features['distance'], labels)\n",
    "plt.xlabel(\"distance\")\n",
    "plt.ylabel(\"estimated time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Handle outliers\n",
    "Before mentioning how outliers can be handled, I want to state that the best way to detect the outliers is to demonstrate the data visually. All other statistical methodologies are open to making mistakes, whereas visualizing the outliers gives a chance to take a decision with high precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exo: remove all the points which have estimated time > 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exo: re-print the chart to confirm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note: </b> at this stage and with the informations that we have, we can do features selection again, and that for choosing <b> the best </b> of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Encode data\n",
    "One-hot encoding is one of the most common encoding methods in machine learning. This method spreads the values in a column to multiple flag columns and assigns 0 or 1 to them. These binary values express the relationship between grouped and encoded column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode data\n",
    "def oneHotEncode(df, col):\n",
    "    dfDummies = pd.get_dummies(df[col], prefix = col)\n",
    "    df = pd.concat([df, dfDummies], axis=1)\n",
    "    \n",
    "    # write youe code here\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exo: complete the oneHotEncode function (drop the current column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the function to our data\n",
    "features = oneHotEncode(features, 'travel_type')\n",
    "features = oneHotEncode(features, 'car_type')\n",
    "features = oneHotEncode(features, 'day_name')\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Scaling\n",
    "In most cases, the numerical features of the dataset do not have a certain range and they differ from each other. In real life, it is nonsense to expect age and income columns to have the same range. But from the machine learning point of view, how these two columns can be compared?  \n",
    "<b> Scaling </b> solves this problem. The continuous features become identical in terms of the range, after a scaling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale data\n",
    "def scale(df, cols):     \n",
    "    for col in cols:\n",
    "        \n",
    "        # write your code here: \n",
    "    return df\n",
    "features = scale(features, [\"distance\", \"hour\"]) \n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exo: apply min-max scaling in the previous function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Conclusion\n",
    "\n",
    "You are now capable to exploit data and extract the most useful informations from it.\n",
    "\n",
    "in the next lesson you will learn how to use the our final data to create and train a model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
